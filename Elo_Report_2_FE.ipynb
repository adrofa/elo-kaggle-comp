{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"header.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <h1>Report on Participation in Kaggle Competition</h1>\n",
    "    <h1>Part II: Feature Engineering</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><i>\n",
    "In this Jupyter notebook I am engineering features from 4 data sheets provided by competition originator.<br/>\n",
    "<br/>\n",
    "Prepared by Artem Drofa.\n",
    "</i></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='main_takeaways'></a>\n",
    "## Main Takeaways\n",
    "1. Categorical data in train and test data sheets have been one-hot encoded.\n",
    "2. Historical Transactions have been splitted into authorized and denied (no denied among New Merchant transactions), finally identical feature have been engineered out of 3 data sheets:\n",
    "   * historical authorized transactions;\n",
    "   * historical denied transactions;\n",
    "   * new merchant transactions.\n",
    "3. In mentioned transactions data sheets:\n",
    "   * numerical columns have been aggregated by card_id and following values have been estimated: sum, mean, min, max and var values;\n",
    "   * categorical columns have been one-hot encoded, then aggregated by card_id and following values have been estimated: sum and sum devided on amount of transactions performed by card_id (i.e. share of transactions in each category). <font color='blue'><i>(Due to limited capacity of my laptop I had to perform this operation using sparse matrix multiplication (not pandas aggregation), may be, this 'trick' will be interesting to dear reader)</i></font>;\n",
    "   * results have been merged with train and test data sheets (key = card_id).\n",
    "4. Cosine distance between historical and new merchant transactions has been added for trying to identify influence of Elo's recomendation algorithms on loyalty score.\n",
    "5. Data sheets prepared for model training:\n",
    "   * train_df.pkl;\n",
    "   * test_df.pkl;\n",
    "   * target_df.pkl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from scipy.spatial import distance\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "from IPython.core import display as ICD\n",
    "\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = '.../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "# Reduce Memory Function\n",
    "# (https://www.kaggle.com/roydatascience/elo-stack-interactions-on-categorical-variables)\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in tqdm(df.columns):\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'\n",
    "                      .format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Data Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATA, 'train.pkl'), 'rb') as pkl_f:\n",
    "    train = pickle.load(pkl_f)\n",
    "with open(os.path.join(PATH_TO_DATA, 'test.pkl'), 'rb') as pkl_f:\n",
    "    test = pickle.load(pkl_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test data sheets preview:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>card_id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C_ID_92a2005557</th>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>C_ID_92a2005557</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.820283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_3d0044924f</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>C_ID_3d0044924f</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.392913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_d639edf6cd</th>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>C_ID_d639edf6cd</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_186d6a6901</th>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>C_ID_186d6a6901</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.142495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_cdbd2c0db2</th>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>C_ID_cdbd2c0db2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.159749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                first_active_month          card_id  feature_1  feature_2  \\\n",
       "card_id                                                                     \n",
       "C_ID_92a2005557         2017-06-01  C_ID_92a2005557          5          2   \n",
       "C_ID_3d0044924f         2017-01-01  C_ID_3d0044924f          4          1   \n",
       "C_ID_d639edf6cd         2016-08-01  C_ID_d639edf6cd          2          2   \n",
       "C_ID_186d6a6901         2017-09-01  C_ID_186d6a6901          4          3   \n",
       "C_ID_cdbd2c0db2         2017-11-01  C_ID_cdbd2c0db2          1          3   \n",
       "\n",
       "                 feature_3    target  \n",
       "card_id                               \n",
       "C_ID_92a2005557          1 -0.820283  \n",
       "C_ID_3d0044924f          0  0.392913  \n",
       "C_ID_d639edf6cd          0  0.688056  \n",
       "C_ID_186d6a6901          0  0.142495  \n",
       "C_ID_cdbd2c0db2          0 -0.159749  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>card_id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C_ID_0ab67a22ab</th>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>C_ID_0ab67a22ab</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_130fd0cbdd</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>C_ID_130fd0cbdd</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_b709037bc5</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>C_ID_b709037bc5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_d27d835a9f</th>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>C_ID_d27d835a9f</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_2b5e3df5c2</th>\n",
       "      <td>2015-12-01</td>\n",
       "      <td>C_ID_2b5e3df5c2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                first_active_month          card_id  feature_1  feature_2  \\\n",
       "card_id                                                                     \n",
       "C_ID_0ab67a22ab         2017-04-01  C_ID_0ab67a22ab          3          3   \n",
       "C_ID_130fd0cbdd         2017-01-01  C_ID_130fd0cbdd          2          3   \n",
       "C_ID_b709037bc5         2017-08-01  C_ID_b709037bc5          5          1   \n",
       "C_ID_d27d835a9f         2017-12-01  C_ID_d27d835a9f          2          1   \n",
       "C_ID_2b5e3df5c2         2015-12-01  C_ID_2b5e3df5c2          5          1   \n",
       "\n",
       "                 feature_3  \n",
       "card_id                     \n",
       "C_ID_0ab67a22ab          1  \n",
       "C_ID_130fd0cbdd          0  \n",
       "C_ID_b709037bc5          1  \n",
       "C_ID_d27d835a9f          0  \n",
       "C_ID_2b5e3df5c2          1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ICD.display(train.head())\n",
    "ICD.display(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = train.append(test, sort=False)\n",
    "del train, test\n",
    "gc.collect()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first_active_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of unique first_active_month: 77\n"
     ]
    }
   ],
   "source": [
    "print('# of unique first_active_month:', train_test['first_active_month'].unique().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd like to represent first_active_month as a difference in days from some day in the future (fut_day). Let' take max(first_active_date, purchase_date)+1 as fut_day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open hist_ and new_ -trans data sheets\n",
    "with open(os.path.join(PATH_TO_DATA, 'hist_trans.pkl'), 'rb') as pkl_f:\n",
    "    hist_trans = pickle.load(pkl_f)\n",
    "with open(os.path.join(PATH_TO_DATA, 'new_trans.pkl'), 'rb') as pkl_f:\n",
    "    new_trans = pickle.load(pkl_f)\n",
    "\n",
    "#hist_ and new_ -trans (purchase) dates\n",
    "htd = hist_trans['purchase_date']\n",
    "ntd = new_trans['purchase_date']\n",
    "\n",
    "#hist_ and new_ -trans data sheets utilize a lot memory, let's clear it for now\n",
    "del hist_trans, new_trans\n",
    "gc.collect()\n",
    "\n",
    "#creating day in the future\n",
    "fut_day = max(train_test['first_active_month'].max(), htd.max(), ntd.max()) + pd.DateOffset(1)\n",
    "fut_day = pd.to_datetime(fut_day.strftime('%Y-%m-%d'))\n",
    "\n",
    "#adding to train_test\n",
    "train_test['first_active_month_age'] = (fut_day - train_test['first_active_month']).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature_1, feature_2, feature_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values:\n",
      "- feature_1 [5 4 2 1 3]\n",
      "- feature_2 [2 1 3]\n",
      "- feature_3 [1 0]\n"
     ]
    }
   ],
   "source": [
    "print('Unique values:')\n",
    "for f in ['feature_1', 'feature_2', 'feature_3']:\n",
    "    print('-', f, train_test[f].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature_1 and feature_2 are ctegorical (anonymized), number of unique values is above 2, thus I'll one-hot encode this features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in ['feature_1', 'feature_2']:\n",
    "    dummies = pd.get_dummies(train_test[f], prefix=f, drop_first=True)\n",
    "    train_test = train_test.merge(dummies, how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>card_id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>target</th>\n",
       "      <th>first_active_month_age</th>\n",
       "      <th>feature_1_2</th>\n",
       "      <th>feature_1_3</th>\n",
       "      <th>feature_1_4</th>\n",
       "      <th>feature_1_5</th>\n",
       "      <th>feature_2_2</th>\n",
       "      <th>feature_2_3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>card_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C_ID_92a2005557</th>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>C_ID_92a2005557</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.820283</td>\n",
       "      <td>334</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_3d0044924f</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>C_ID_3d0044924f</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.392913</td>\n",
       "      <td>485</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_d639edf6cd</th>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>C_ID_d639edf6cd</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688056</td>\n",
       "      <td>638</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_186d6a6901</th>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>C_ID_186d6a6901</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.142495</td>\n",
       "      <td>242</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_ID_cdbd2c0db2</th>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>C_ID_cdbd2c0db2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.159749</td>\n",
       "      <td>181</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                first_active_month          card_id  feature_1  feature_2  \\\n",
       "card_id                                                                     \n",
       "C_ID_92a2005557         2017-06-01  C_ID_92a2005557          5          2   \n",
       "C_ID_3d0044924f         2017-01-01  C_ID_3d0044924f          4          1   \n",
       "C_ID_d639edf6cd         2016-08-01  C_ID_d639edf6cd          2          2   \n",
       "C_ID_186d6a6901         2017-09-01  C_ID_186d6a6901          4          3   \n",
       "C_ID_cdbd2c0db2         2017-11-01  C_ID_cdbd2c0db2          1          3   \n",
       "\n",
       "                 feature_3    target  first_active_month_age  feature_1_2  \\\n",
       "card_id                                                                     \n",
       "C_ID_92a2005557          1 -0.820283                     334            0   \n",
       "C_ID_3d0044924f          0  0.392913                     485            0   \n",
       "C_ID_d639edf6cd          0  0.688056                     638            1   \n",
       "C_ID_186d6a6901          0  0.142495                     242            0   \n",
       "C_ID_cdbd2c0db2          0 -0.159749                     181            0   \n",
       "\n",
       "                 feature_1_3  feature_1_4  feature_1_5  feature_2_2  \\\n",
       "card_id                                                               \n",
       "C_ID_92a2005557            0            0            1            1   \n",
       "C_ID_3d0044924f            0            1            0            0   \n",
       "C_ID_d639edf6cd            0            0            0            1   \n",
       "C_ID_186d6a6901            0            1            0            0   \n",
       "C_ID_cdbd2c0db2            0            0            0            0   \n",
       "\n",
       "                 feature_2_3  \n",
       "card_id                       \n",
       "C_ID_92a2005557            0  \n",
       "C_ID_3d0044924f            0  \n",
       "C_ID_d639edf6cd            0  \n",
       "C_ID_186d6a6901            1  \n",
       "C_ID_cdbd2c0db2            1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving and deleting train_test data sheet for memory clearing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATA, 'train_test.pkl'), 'wb') as pkl_f:\n",
    "    pickle.dump(train_test, pkl_f)\n",
    "\n",
    "del train_test\n",
    "gc.collect()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical and New Merchant Transactions Data Sheets\n",
    "1. Non-categorical columns;\n",
    "2. Categorical columns;\n",
    "3. Cosine distance between hist and new merchant transaction to reflect activation of Elo's recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATA, 'hist_trans.pkl'), 'rb') as pkl_f:\n",
    "    hist_trans = pickle.load(pkl_f)\n",
    "with open(os.path.join(PATH_TO_DATA, 'new_trans.pkl'), 'rb') as pkl_f:\n",
    "    new_trans = pickle.load(pkl_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, I split hist_trans data sheet into (1) hist_trans with authorized transactions and (2) hist_trans with denied transactions.<br/>\n",
    "<i>New_trans data sheet doesn't include unauthorized transactions.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_trans_auth = hist_trans[hist_trans['authorized_flag'] == 1]\n",
    "hist_trans_den = hist_trans[hist_trans['authorized_flag'] == 0]\n",
    "\n",
    "#indexing\n",
    "for ds in [hist_trans_auth, hist_trans_den, new_trans]:\n",
    "    ds.index = ds['card_id']\n",
    "\n",
    "#memory clearing\n",
    "del hist_trans\n",
    "gc.collect()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Categorical Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are following non-categorical columns in hist_ and new_ -trans data sheets:\n",
    "* installments;\n",
    "* month_lag;\n",
    "* purchase_amount;\n",
    "* purchase_date;\n",
    "* ref_date_1MY (quasi-linear combination of purchase_date and month_lag).\n",
    "\n",
    "Basing on this columns I'll create additional columns:\n",
    "* purchase_amount / installments;\n",
    "* purchase_date_age (difference between a date from future and purchase_date);\n",
    "* ref_date_1MY_age (difference between a date from future and purchase_date).\n",
    "\n",
    "After this purchase_date and ref_date_1MY could be dropped. All non-categorical columns will be aggregated by crad_id resulting in sum, min, max, mean and variance values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating additional columns\n",
    "for ds in [hist_trans_auth, hist_trans_den, new_trans]:\n",
    "    \n",
    "    ds['purchase_amount_per_installments'] =\\\n",
    "    ds['purchase_amount'] / ds['installments'].replace([0, 999], np.nan)\n",
    "    \n",
    "    ds['purchase_date_age'] = (fut_day - ds['purchase_date']).dt.total_seconds()\n",
    "    ds['ref_date_1MY_age'] = (fut_day - ds['ref_date_1MY']).dt.total_seconds()\n",
    "    \n",
    "    ds = ds.drop(columns=['purchase_date', 'ref_date_1MY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns to aggregate\n",
    "non_cat_col = ['installments', 'month_lag', 'purchase_amount',\n",
    "               'purchase_date_age', 'ref_date_1MY_age', 'purchase_amount_per_installments']\n",
    "\n",
    "#aggregation parameters\n",
    "aggs = {}\n",
    "for col in non_cat_col:\n",
    "    aggs[col] = ['sum', 'max', 'min', 'mean', 'var']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_non_cat():\n",
    "    #open train_test for merging\n",
    "    with open(os.path.join(PATH_TO_DATA, 'train_test.pkl'), 'rb') as pkl_f:\n",
    "        train_test = pickle.load(pkl_f)\n",
    "\n",
    "    for ds, ds_name in tqdm(zip([hist_trans_auth, hist_trans_den, new_trans], ['ht_auth', 'ht_den', 'nt'])):\n",
    "        #aggregating\n",
    "        ds_agg = ds.reset_index(drop=True).groupby('card_id').agg(aggs)\n",
    "\n",
    "        #renaming columns\n",
    "        ds_agg.columns = [ds_name+'_'+i+'_'+j for i, j in zip(ds_agg.columns.get_level_values(0),\n",
    "                                                              ds_agg.columns.get_level_values(1))]\n",
    "        #merging\n",
    "        train_test = train_test.merge(ds_agg, how='left', left_index=True, right_index=True)\n",
    "\n",
    "        #memory clearing\n",
    "        del ds_agg\n",
    "        gc.collect()\n",
    "\n",
    "    #saving train_test and clearing memory\n",
    "    with open(os.path.join(PATH_TO_DATA, 'train_test.pkl'), 'wb') as pkl_f:\n",
    "        pickle.dump(train_test, pkl_f)\n",
    "    del train_test\n",
    "    gc.collect()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:50, 23.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.6 s, sys: 11.6 s, total: 33.2 s\n",
      "Wall time: 51.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_test_non_cat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look at categorical columns in hist_ and new_ -trans data sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['city_id', 'category_1', 'category_3', 'merchant_category_id',\n",
    "           'merchant_id', 'category_2', 'state_id', 'subsector_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city_id : 308\n",
      "category_1 : 2\n",
      "category_3 : 4\n",
      "merchant_category_id : 327\n",
      "merchant_id : 324323\n",
      "category_2 : 6\n",
      "state_id : 25\n",
      "subsector_id : 41\n"
     ]
    }
   ],
   "source": [
    "for col in cat_cols:\n",
    "    values = hist_trans_auth[col]\n",
    "    for ds in [hist_trans_den, new_trans]:\n",
    "        values.append(ds[col])\n",
    "    print(col, ':', values.unique().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All columns beside category_1 and merchant_id will be one-hot encoded. After I will aggregate columns by card_id and estimate sum and mean of each column: sum will show total amount of transactions in each category and mean will show share of transactions in each category.\n",
    "* Category_1 has only 2 unique values, it should be binarized, mean and sum should be aggregated.\n",
    "* Merchant_id has to much values for one-hot encoding, hence only number of unique values for each card_id will be added to train_test;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One-Hot Encoding and Aggreation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately my laptop is not able to perform one-hot encoding and aggregate sums and means using pandas, hence <b>I transform data using scipy OneHotEncoder and aggregate it using sparse matrix multiplication</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_cols = ['city_id', 'category_3', 'merchant_category_id', 'category_2', 'state_id', 'subsector_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:57, 26.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.9 s, sys: 7.46 s, total: 19.4 s\n",
      "Wall time: 59.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#transactions (files) dump\n",
    "for ds, ds_name in tqdm(zip([hist_trans_auth, hist_trans_den, new_trans], ['ht_auth', 'ht_den', 'nt'])):\n",
    "    with open(os.path.join(PATH_TO_DATA, ds_name+'.pkl'), 'wb') as pkl_f:\n",
    "        pickle.dump(ds, pkl_f)\n",
    "\n",
    "#clearing memory\n",
    "del hist_trans_auth, hist_trans_den, new_trans\n",
    "gc.collect()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell I:\n",
    "* perform one-hot encoding for each of selected feature in each of 3 data sheets (historical transaction authorized, historical transactions denied, new merchant transasctions;\n",
    "* agregate received columns by card_id and estimate sum and mean using sparse matrix multiplication;\n",
    "* save received matrices.\n",
    "\n",
    "After I will stack all matrices for each data sheet and merge it by card_id with train_test data sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe_agg():\n",
    "    for ds_name in ['ht_auth', 'ht_den', 'nt']:\n",
    "        #open data sheet\n",
    "        with open(os.path.join(PATH_TO_DATA, ds_name+'.pkl'), 'rb') as pkl_f:\n",
    "            ds = pickle.load(pkl_f)\n",
    "\n",
    "        #STEP I: preparation of card_id matrix for further multiplication for aggregation purposes \n",
    "        card_id = ds['card_id']\n",
    "        card_id_enc = OneHotEncoder()\n",
    "        card_id_ohe = card_id_enc.fit_transform(card_id.values.reshape(-1, 1))\n",
    "\n",
    "        vector_slicer = np.vectorize(lambda a: a[3:])\n",
    "        card_id_index = vector_slicer(card_id_enc.get_feature_names())\n",
    "        \n",
    "        #STEP II: saving card_id index\n",
    "        with open(os.path.join(PATH_TO_DATA, ds_name+'_card_id.pkl'), 'wb') as pkl_f:\n",
    "                pickle.dump(card_id_index, pkl_f)\n",
    "\n",
    "        #memory clearing\n",
    "        del card_id, card_id_enc\n",
    "        del vector_slicer, card_id_index\n",
    "        gc.collect()\n",
    "\n",
    "        #STEP III: columns encoding\n",
    "        for col in tqdm(ohe_cols):\n",
    "            col_enc = OneHotEncoder()\n",
    "            col_ohe = col_enc.fit_transform(ds[col].astype(str).fillna('NaN').values.reshape(-1, 1))\n",
    "            col_names = col_enc.get_feature_names(input_features=[col])\n",
    "\n",
    "            #STEP IV: aggregation\n",
    "            col_sum = (col_ohe.T @ card_id_ohe).T #SUM\n",
    "            col_mean = csr_matrix(col_sum / card_id_ohe.sum(axis=0).T) #MEAN (i.e. share)\n",
    "\n",
    "            #STEP VI: Aggregating sparse matrices and columns names\n",
    "            col_agg = hstack([col_sum, col_mean]) #index=card_id_index,\n",
    "            col_names_agg = [ds_name+'_'+col_name+'_sum' for col_name in col_names] +\\\n",
    "                            [ds_name+'_'+col_name+'_mean' for col_name in col_names]\n",
    "\n",
    "\n",
    "            #STEP VII: saving transformed features and its names\n",
    "            with open(os.path.join(PATH_TO_DATA, ds_name+'_'+col+'_ohe_features.pkl'), 'wb') as pkl_f:\n",
    "                pickle.dump(col_agg, pkl_f)\n",
    "            with open(os.path.join(PATH_TO_DATA, ds_name+'_'+col+'_ohe_features_names.pkl'), 'wb') as pkl_f:\n",
    "                pickle.dump(col_names_agg, pkl_f)\n",
    "\n",
    "            #memory clearing\n",
    "            del col_enc, col_ohe, col_names\n",
    "            del col_sum, col_mean\n",
    "            del col_agg, col_names_agg\n",
    "            gc.collect()\n",
    "        \n",
    "        #memory clearing\n",
    "        del card_id_ohe\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [02:01<00:00, 21.13s/it]\n",
      "100%|██████████| 6/6 [00:14<00:00,  2.47s/it]\n",
      "100%|██████████| 6/6 [00:13<00:00,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 32s, sys: 34.1 s, total: 3min 7s\n",
      "Wall time: 3min 8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ohe_agg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below I h-stack all received matrices into 1 DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe_hstack():\n",
    "    for ds_name in ['ht_auth', 'ht_den', 'nt']:\n",
    "\n",
    "        #STEP I: initializing matrix with features and column names for it with 1st feature\n",
    "        with open(os.path.join(PATH_TO_DATA, ds_name+'_'+ohe_cols[0]+'_ohe_features.pkl'), 'rb') as pkl_f:\n",
    "                col_agg_full = pickle.load(pkl_f)\n",
    "        with open(os.path.join(PATH_TO_DATA, ds_name+'_'+ohe_cols[0]+'_ohe_features_names.pkl'), 'rb') as pkl_f:\n",
    "                col_names_full = pickle.load(pkl_f)\n",
    "\n",
    "        #STEP II: h-stacking all matrices and column names\n",
    "        for col in tqdm(ohe_cols[1:]):\n",
    "            with open(os.path.join(PATH_TO_DATA, ds_name+'_'+col+'_ohe_features.pkl'), 'rb') as pkl_f:\n",
    "                col_agg = pickle.load(pkl_f)\n",
    "            with open(os.path.join(PATH_TO_DATA, ds_name+'_'+col+'_ohe_features_names.pkl'), 'rb') as pkl_f:\n",
    "                col_names = pickle.load(pkl_f)\n",
    "\n",
    "            col_agg_full = hstack([col_agg_full, col_agg])\n",
    "            col_names_full += col_names\n",
    "\n",
    "            #memory clearing\n",
    "            del col_agg, col_names\n",
    "            gc.collect()\n",
    "\n",
    "        #STEP III: open card_id index\n",
    "        with open(os.path.join(PATH_TO_DATA, ds_name+'_card_id.pkl'), 'rb') as pkl_f:\n",
    "            card_id = pickle.load(pkl_f)\n",
    "\n",
    "        #STEP IV: DataFrame creation and saving\n",
    "        df = pd.DataFrame(col_agg_full.todense(), index=card_id, columns=col_names_full)\n",
    "        with open(os.path.join(PATH_TO_DATA, ds_name+'_ohe_df.pkl'), 'wb') as pkl_f:\n",
    "                pickle.dump(df, pkl_f)\n",
    "\n",
    "        #memory clearing\n",
    "        del df\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:02<00:00,  2.40it/s]\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.36it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 s, sys: 20.7 s, total: 34.6 s\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ohe_hstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting support files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_sup_files():\n",
    "    for ds_name in tqdm(['ht_auth', 'ht_den', 'nt']):\n",
    "\n",
    "        for col in ohe_cols:\n",
    "            os.remove(os.path.join(PATH_TO_DATA, ds_name+'_'+col+'_ohe_features.pkl'))\n",
    "            os.remove(os.path.join(PATH_TO_DATA, ds_name+'_'+col+'_ohe_features_names.pkl'))\n",
    "        \n",
    "        os.remove(os.path.join(PATH_TO_DATA, ds_name+'_card_id.pkl'))\n",
    "\n",
    "    gc.collect()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 309.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.4 ms, sys: 0 ns, total: 48.4 ms\n",
      "Wall time: 54.4 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "del_sup_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing memory usage of created DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_ohe_df():\n",
    "    for ds_name in ['ht_auth', 'ht_den', 'nt']:\n",
    "\n",
    "        #open df\n",
    "        with open(os.path.join(PATH_TO_DATA, ds_name+'_ohe_df.pkl'), 'rb') as pkl_f:\n",
    "            df = pickle.load(pkl_f)\n",
    "\n",
    "        #reduce memory usage\n",
    "        df = reduce_mem_usage(df)\n",
    "\n",
    "        #save df\n",
    "        with open(os.path.join(PATH_TO_DATA, ds_name+'_ohe_df.pkl'), 'wb') as pkl_f:\n",
    "            pickle.dump(df, pkl_f)\n",
    "\n",
    "        #memory clearing\n",
    "        del df\n",
    "        gc.collect()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1422/1422 [14:19<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 885.43 Mb (74.9% reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1390/1390 [11:10<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 733.05 Mb (74.9% reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1396/1396 [11:47<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 774.39 Mb (74.9% reduction)\n",
      "CPU times: user 22min 5s, sys: 51min 40s, total: 1h 13min 45s\n",
      "Wall time: 38min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reduce_mem_ohe_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below I merge all of created DataFrames with train_test data sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_train_test_merge():\n",
    "    with open(os.path.join(PATH_TO_DATA, 'train_test.pkl'), 'rb') as pkl_f:\n",
    "        train_test = pickle.load(pkl_f)\n",
    "\n",
    "    for ds_name in tqdm(['ht_auth', 'ht_den', 'nt']):\n",
    "\n",
    "        #open df\n",
    "        with open(os.path.join(PATH_TO_DATA, ds_name+'_ohe_df.pkl'), 'rb') as pkl_f:\n",
    "            df = pickle.load(pkl_f)\n",
    "\n",
    "        #merge train_test with df\n",
    "        train_test = train_test.merge(df, how='left', left_index=True, right_index=True)\n",
    "\n",
    "        #memory clearing\n",
    "        del df\n",
    "        gc.collect()\n",
    "\n",
    "    #saving train_test and clearing memory\n",
    "    with open(os.path.join(PATH_TO_DATA, 'train_test.pkl'), 'wb') as pkl_f:\n",
    "        pickle.dump(train_test, pkl_f)\n",
    "    del train_test\n",
    "    gc.collect()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:26<00:00,  8.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.3 s, sys: 8.06 s, total: 24.3 s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_train_test_merge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing train_test memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_train_test():\n",
    "    #open train_test\n",
    "    with open(os.path.join(PATH_TO_DATA, 'train_test.pkl'), 'rb') as pkl_f:\n",
    "        train_test = pickle.load(pkl_f)\n",
    "\n",
    "    #reduce memory usage\n",
    "    train_test = reduce_mem_usage(train_test)\n",
    "\n",
    "    #save train_test\n",
    "    with open(os.path.join(PATH_TO_DATA, 'train_test.pkl'), 'wb') as pkl_f:\n",
    "        pickle.dump(train_test, pkl_f)\n",
    "\n",
    "    #memory clearing\n",
    "    del train_test\n",
    "    gc.collect()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4311/4311 [00:49<00:00, 87.19it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 2699.44 Mb (5.4% reduction)\n",
      "CPU times: user 52.8 s, sys: 4.71 s, total: 57.5 s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reduce_mem_train_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### category_1 and merchant_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_1_merchant_id():\n",
    "    with open(os.path.join(PATH_TO_DATA, 'train_test.pkl'), 'rb') as pkl_f:\n",
    "        train_test = pickle.load(pkl_f)\n",
    "\n",
    "    aggs_2 = {'category_1':['sum', 'mean'],\n",
    "            'merchant_id':['nunique']}\n",
    "\n",
    "    for ds_name in tqdm(['ht_auth', 'ht_den', 'nt']):\n",
    "\n",
    "        #open data sheet\n",
    "        with open(os.path.join(PATH_TO_DATA, ds_name+'.pkl'), 'rb') as pkl_f:\n",
    "            ds = pickle.load(pkl_f)\n",
    "\n",
    "        #binarization of category_1\n",
    "        ds['category_1'] = ds['category_1'].map({'Y':1, 'N':0})\n",
    "\n",
    "        #aggregating\n",
    "        ds_agg = ds.reset_index(drop=True).groupby('card_id').agg(aggs_2)\n",
    "\n",
    "        #renaming columns\n",
    "        ds_agg.columns = [ds_name+'_'+i+'_'+j for i, j in zip(ds_agg.columns.get_level_values(0),\n",
    "                                                              ds_agg.columns.get_level_values(1))]\n",
    "\n",
    "        #merging\n",
    "        train_test = train_test.merge(ds_agg, how='left', left_index=True, right_index=True)\n",
    "\n",
    "        #clearing memory\n",
    "        del ds, ds_agg\n",
    "        gc.collect()\n",
    "\n",
    "    #saving train_test and clearing memory\n",
    "    with open(os.path.join(PATH_TO_DATA, 'train_test.pkl'), 'wb') as pkl_f:\n",
    "        pickle.dump(train_test, pkl_f)\n",
    "    del train_test\n",
    "    gc.collect()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [02:59<00:00, 85.53s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.9 s, sys: 33 s, total: 1min 21s\n",
      "Wall time: 3min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "category_1_merchant_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would like to remind that historical and new merchant transactions are chronologically splitted by some event, supposingly, this event is a launch of Elo's merchant reccomendation algorithm. New merchants transaction data sheet includes information only about transations in merchants which have been never met before for each card_id.\n",
    "\n",
    "The idea, which lies on a surface, is to recomend to a client new merchants from merchant categories in which this client has most of his transactions (by quantity or sum).\n",
    "\n",
    "Basing on these I'd like to create 2 additional features for each card_id:\n",
    "1. cosine distance between vectors of <u>quantity</u> of purchases aggregated by merchant_category_id from historical authorized and new merchant transactions;\n",
    "2. cosine distance between vectors of <u>sum</u> of purchases aggregated by merchant_category_id from historical authorized and new merchant transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quantity of Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_dist_quantity():\n",
    "\n",
    "    #opening transactions files\n",
    "    with open(os.path.join(PATH_TO_DATA, 'ht_auth.pkl'), 'rb') as pkl_f:\n",
    "        ht_auth = pickle.load(pkl_f)\n",
    "    with open(os.path.join(PATH_TO_DATA, 'nt.pkl'), 'rb') as pkl_f:\n",
    "        nt = pickle.load(pkl_f)\n",
    "    print('files opened')\n",
    "\n",
    "    #generating vectors\n",
    "    ht_vec = ht_auth.reset_index(drop=True).groupby(['card_id', 'merchant_category_id']\n",
    "                                                   )['merchant_category_id'].size().unstack(fill_value=0)\n",
    "    nt_vec = nt.reset_index(drop=True).groupby(['card_id', 'merchant_category_id']\n",
    "                                              )['merchant_category_id'].size().unstack(fill_value=0)\n",
    "    print('vectors generated')\n",
    "\n",
    "    #clearing memory\n",
    "    del ht_auth, nt\n",
    "    gc.collect()\n",
    "\n",
    "    #finding out missing merchant categories in vecs\n",
    "    ht_miss_cat = np.setdiff1d(nt_vec.columns, ht_vec.columns)\n",
    "    nt_miss_cat = np.setdiff1d(ht_vec.columns, nt_vec.columns)\n",
    "    print('missing categories found')\n",
    "\n",
    "    #fullfilling missing merchant categories\n",
    "    for cat in ht_miss_cat:\n",
    "        ht_vec[cat] = 0\n",
    "    for cat in nt_miss_cat:\n",
    "        nt_vec[cat] = 0\n",
    "    print('missing categories fullfilled')\n",
    "\n",
    "    #sorting columns\n",
    "    cols_sorted = np.sort(ht_vec.columns)\n",
    "    print('columns sorted')\n",
    "\n",
    "    ht_vec = ht_vec[cols_sorted]\n",
    "    nt_vec = nt_vec[cols_sorted]\n",
    "\n",
    "    #renaming columns\n",
    "    ht_vec.columns = ['ht_'+str(col) for col in cols_sorted]\n",
    "    nt_vec.columns = ['nt_'+str(col) for col in cols_sorted]\n",
    "    print('columns renamed')\n",
    "\n",
    "    #merging vecs and fill NaNs with 0\n",
    "    ht_nt_vec = pd.concat([ht_vec, nt_vec], axis=1, sort=False).fillna(0)\n",
    "    print('vecs merged, NaNs filled')\n",
    "\n",
    "    #estimating cosine distance\n",
    "    dot_product = np.einsum('ij,ij->i', ht_nt_vec[ht_vec.columns], ht_nt_vec[nt_vec.columns])\n",
    "    ht_vec_mod = (ht_nt_vec[ht_vec.columns] ** 2).sum(axis=1) ** (1/2)\n",
    "    nt_vec_mod = (ht_nt_vec[nt_vec.columns] ** 2).sum(axis=1) ** (1/2)\n",
    "    ht_nt_vec['cos_dist_count_by_merchant_category'] = 1 - dot_product / (ht_vec_mod * nt_vec_mod)\n",
    "    print('cosine distance estimated')\n",
    "\n",
    "    #adding created feature to train_test\n",
    "    with open(os.path.join(PATH_TO_DATA, 'train_test.pkl'), 'rb') as pkl_f:\n",
    "        train_test = pickle.load(pkl_f)\n",
    "    print('train_test opened')\n",
    "\n",
    "    train_test = train_test.merge(ht_nt_vec[['cos_dist_count_by_merchant_category']],\n",
    "                                  how='left', left_index=True, right_index=True)\n",
    "    print('features added')\n",
    "\n",
    "    with open(os.path.join(PATH_TO_DATA, 'train_test.pkl'), 'wb') as pkl_f:\n",
    "        pickle.dump(train_test, pkl_f)\n",
    "    print('train_test dumped')\n",
    "\n",
    "    #memory clearing\n",
    "    del train_test, ht_vec, nt_vec, ht_nt_vec\n",
    "    gc.collect()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files opened\n",
      "vectors generated\n",
      "missing categories found\n",
      "missing categories fullfilled\n",
      "columns sorted\n",
      "columns renamed\n",
      "vecs merged, NaNs filled\n",
      "cosine distance estimated\n",
      "train_test opened\n",
      "features added\n",
      "train_test dumped\n",
      "CPU times: user 38.2 s, sys: 26.9 s, total: 1min 5s\n",
      "Wall time: 2min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cos_dist_quantity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quantity of Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_dist_purchase_amount():\n",
    "    \n",
    "    #opening transactions files\n",
    "    with open(os.path.join(PATH_TO_DATA, 'ht_auth.pkl'), 'rb') as pkl_f:\n",
    "        ht_auth = pickle.load(pkl_f)\n",
    "    with open(os.path.join(PATH_TO_DATA, 'nt.pkl'), 'rb') as pkl_f:\n",
    "        nt = pickle.load(pkl_f)\n",
    "    print('files opened')\n",
    "    \n",
    "    #generating vectors\n",
    "    ht_vec = ht_auth.reset_index(drop=True).groupby(['card_id', 'merchant_category_id']\n",
    "                                                   )['purchase_amount'].sum().unstack(fill_value=0)\n",
    "    nt_vec = nt.reset_index(drop=True).groupby(['card_id', 'merchant_category_id']\n",
    "                                              )['purchase_amount'].sum().unstack(fill_value=0)\n",
    "    print('vectors generated')\n",
    "    \n",
    "    #clearing memory\n",
    "    del ht_auth, nt\n",
    "    gc.collect()\n",
    "\n",
    "    #finding out missing merchant categories in vecs\n",
    "    ht_miss_cat = np.setdiff1d(nt_vec.columns, ht_vec.columns)\n",
    "    nt_miss_cat = np.setdiff1d(ht_vec.columns, nt_vec.columns)\n",
    "    print('missing categories found')\n",
    "\n",
    "    #fullfilling missing merchant categories\n",
    "    for cat in ht_miss_cat:\n",
    "        ht_vec[cat] = 0\n",
    "    for cat in nt_miss_cat:\n",
    "        nt_vec[cat] = 0\n",
    "    print('missing categories fullfilled')\n",
    "\n",
    "    #sorting columns\n",
    "    cols_sorted = np.sort(ht_vec.columns)\n",
    "    print('columns sorted')\n",
    "\n",
    "    ht_vec = ht_vec[cols_sorted]\n",
    "    nt_vec = nt_vec[cols_sorted]\n",
    "\n",
    "    #renaming columns\n",
    "    ht_vec.columns = ['ht_'+str(col) for col in cols_sorted]\n",
    "    nt_vec.columns = ['nt_'+str(col) for col in cols_sorted]\n",
    "    print('columns renamed')\n",
    "\n",
    "    #merging vecs and fill NaNs with 0\n",
    "    ht_nt_vec = pd.concat([ht_vec, nt_vec], axis=1, sort=False).fillna(0)\n",
    "    print('vecs merged, NaNs filled')\n",
    "    \n",
    "    #estimating cosine distance\n",
    "    dot_product = np.einsum('ij,ij->i', ht_nt_vec[ht_vec.columns], ht_nt_vec[nt_vec.columns])\n",
    "    ht_vec_mod = (ht_nt_vec[ht_vec.columns] ** 2).sum(axis=1) ** (1/2)\n",
    "    nt_vec_mod = (ht_nt_vec[nt_vec.columns] ** 2).sum(axis=1) ** (1/2)\n",
    "    ht_nt_vec['cos_dist_sum_by_merchant_category'] = 1 - dot_product / (ht_vec_mod * nt_vec_mod)\n",
    "    print('cosine distance estimated')\n",
    "    \n",
    "    #adding created feature to train_test\n",
    "    with open(os.path.join(PATH_TO_DATA, 'train_test.pkl'), 'rb') as pkl_f:\n",
    "        train_test = pickle.load(pkl_f)\n",
    "    print('train_test opened')\n",
    "\n",
    "    train_test = train_test.merge(ht_nt_vec[['cos_dist_sum_by_merchant_category']],\n",
    "                                  how='left', left_index=True, right_index=True)\n",
    "    print('features added')\n",
    "\n",
    "    with open(os.path.join(PATH_TO_DATA, 'train_test.pkl'), 'wb') as pkl_f:\n",
    "        pickle.dump(train_test, pkl_f)\n",
    "    print('train_test dumped')\n",
    "\n",
    "    #memory clearing\n",
    "    del train_test, ht_vec, nt_vec, ht_nt_vec\n",
    "    gc.collect()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files opened\n",
      "vectors generated\n",
      "missing categories found\n",
      "missing categories fullfilled\n",
      "columns sorted\n",
      "columns renamed\n",
      "vecs merged, NaNs filled\n",
      "cosine distance estimated\n",
      "train_test opened\n",
      "features added\n",
      "train_test dumped\n",
      "CPU times: user 47.9 s, sys: 58 s, total: 1min 45s\n",
      "Wall time: 4min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cos_dist_purchase_amount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving prepared <u>train_df</u>, <u>test_df</u> and <u>target_df</u> data sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_saving():\n",
    "    with open(os.path.join(PATH_TO_DATA, 'train_test.pkl'), 'rb') as pkl_f:\n",
    "        train_test = pickle.load(pkl_f)\n",
    "\n",
    "    train = train_test[train_test['target'].notnull()]\n",
    "    test = train_test[train_test['target'].isnull()]\n",
    "\n",
    "    #clearing memory\n",
    "    del train_test\n",
    "    gc.collect()\n",
    "    pass\n",
    "\n",
    "    target = train['target']\n",
    "    \n",
    "    feats_to_drop = ['card_id', 'target', 'first_active_month', 'feature_1', 'feature_2']\n",
    "    train = train.drop(columns=feats_to_drop)\n",
    "    test = test.drop(columns=feats_to_drop)\n",
    "    \n",
    "    for df, df_name in zip([train, test, target], ['train', 'test', 'target']):\n",
    "        with open(os.path.join(PATH_TO_DATA, df_name+'_df.pkl'), 'wb') as pkl_f:\n",
    "            pickle.dump(df, pkl_f)\n",
    "\n",
    "    #clearing memory\n",
    "    del train, test, target\n",
    "    gc.collect()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.4 s, sys: 42.8 s, total: 55.2 s\n",
      "Wall time: 4min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_saving()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Please, consider ['Main Takeaways'](#main_takeaways) at the top of the page."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
